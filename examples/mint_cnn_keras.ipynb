{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## EMNIST with CNN tensorflow backend and keras trainig - \n",
    "\n",
    "### How to get clients to participate in FL training, how to reward them based on their contribution to the utility of the derived model - a mechanism design framework "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our goal is to build here a \"test ramp\" for different formulae of Shapley values. This test ramp would allow us to test different combinations of :\n",
    "- number of agents (the \"clients\", which contributes data and labels for FL training) : a parameter n\n",
    "- EMNIST data distribution between agents : a distribution function f(n) and a skewing parameter $\\zeta$\n",
    "- data quality distribution between agents (by adding noise to the original EMINST data) : a distribution function g(n), a skewing parameter $\\xi$ and a noise parameter $\\gamma \\in [0,1]$ ($\\gamma$ represents the \"messiness\" of the noise we'll be adding, as a proportion of the original data) \n",
    "\n",
    "### Possible extensions :\n",
    "- if data arrives in successive batches in time (timesteps and batches distribution to be defined)\n",
    "\n",
    "For a complete checklist of things to keep in mind on the data/training side please refer to this document : https://docs.google.com/document/d/16bGgYLl2AIN6dsWxfR6WusHu-s6nxooY/edit#heading=h.gjdgxs\n",
    "For a view of our axiomatic approach in redefining Shapley values please refer to this document : https://www.overleaf.com/project/5efb91492eb0ec0001078b73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the FL training  pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-444d951482e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'..'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tempfile, sys, os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "# Import DeepExplain\n",
    "from deepexplain.tensorflow import DeepExplain\n",
    "\n",
    "# Build and train a network.\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 3\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "    \n",
    "    \n",
    "    ### TO BE DONE : add splitting among n players\n",
    "    ### TO BE DONE : distribute data and quality among these n players\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "x_train = (x_train - 0.5) * 2\n",
    "x_test = (x_test - 0.5) * 2\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax')) \n",
    "# ^ IMPORTANT: notice that the final softmax must be in its own layer \n",
    "# if we want to target pre-softmax units\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "    ### TO BE DONE : change this method to FL\n",
    "    \n",
    "    \n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Shapley value computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Here's an exemple of Shapley value computing - this cell here could be replaced by any other Shapley value function - which we will have to code ourself\n",
    "\n",
    "\n",
    "with DeepExplain(session=K.get_session()) as de:  # <-- init DeepExplain context\n",
    "    # Need to reconstruct the graph in DeepExplain context, using the same weights.\n",
    "    # With Keras this is very easy:\n",
    "    # 1. Get the input tensor to the original model\n",
    "    input_tensor = model.layers[0].input\n",
    "    \n",
    "    # 2. We now target the output of the last dense layer (pre-softmax)\n",
    "    # To do so, create a new model sharing the same layers untill the last dense (index -2)\n",
    "    fModel = Model(inputs=input_tensor, outputs = model.layers[-2].output)\n",
    "    target_tensor = fModel(input_tensor)\n",
    "    \n",
    "    xs = x_test[0:10]\n",
    "    ys = y_test[0:10]\n",
    "    \n",
    "    attributions_gradin = de.explain('grad*input', target_tensor, input_tensor, xs, ys=ys)\n",
    "    #attributions_sal   = de.explain('saliency', target_tensor, input_tensor, xs, ys=ys)\n",
    "    #attributions_ig    = de.explain('intgrad', target_tensor, input_tensor, xs, ys=ys)\n",
    "    #attributions_dl    = de.explain('deeplift', target_tensor, input_tensor, xs, ys=ys)\n",
    "    #attributions_elrp  = de.explain('elrp', target_tensor, input_tensor, xs, ys=ys)\n",
    "    #attributions_occ   = de.explain('occlusion', target_tensor, input_tensor, xs, ys=ys)\n",
    "    \n",
    "    # Compare Gradient * Input with approximate Shapley Values\n",
    "    # Note1: Shapley Value sampling with 100 samples per feature (78400 runs) takes a couple of minutes on a GPU.\n",
    "    # Note2: 100 samples are not enough for convergence, the result might be affected by sampling variance\n",
    "    attributions_sv     = de.explain('shapley_sampling', target_tensor, input_tensor, xs, ys=ys, samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the Shapley values - here for the \"total\" data, but should be modified for \"EACH\" client's data\n",
    "\n",
    "\n",
    "from utils import plot, plt\n",
    "%matplotlib inline\n",
    "\n",
    "n_cols = 6\n",
    "n_rows = int(len(attributions_gradin) / 2)\n",
    "fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(3*n_cols, 3*n_rows))\n",
    "\n",
    "for i, (a1, a2) in enumerate(zip(attributions_gradin, attributions_sv)):\n",
    "    row, col = divmod(i, 2)\n",
    "    plot(xs[i].reshape(28, 28), cmap='Greys', axis=axes[row, col*3]).set_title('Original')\n",
    "    plot(a1.reshape(28,28), xi = xs[i], axis=axes[row,col*3+1]).set_title('Grad*Input')\n",
    "    plot(a2.reshape(28,28), xi = xs[i], axis=axes[row,col*3+2]).set_title('Shapley Values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch processing - to be explored later \n",
    "In this example, we generate explanations for the entire testset (10000 images) using the fast Gradient*Input method.\n",
    "`DeepExplain.explain()` accepts the `batch_size` parameter if the data to process does not fit in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with DeepExplain(session=K.get_session()) as de:  # <-- init DeepExplain context\n",
    "    # Need to reconstruct the graph in DeepExplain context, using the same weights.\n",
    "    # With Keras this is very easy:\n",
    "    # 1. Get the input tensor to the original model\n",
    "    input_tensor = model.layers[0].input\n",
    "    \n",
    "    # 2. We now target the output of the last dense layer (pre-softmax)\n",
    "    # To do so, create a new model sharing the same layers untill the last dense (index -2)\n",
    "    fModel = Model(inputs=input_tensor, outputs = model.layers[-2].output)\n",
    "    target_tensor = fModel(input_tensor)\n",
    "    \n",
    "    xs = x_test\n",
    "    ys = y_test\n",
    "    \n",
    "    attributions_gradin = de.explain('grad*input', target_tensor, input_tensor, xs, ys=ys, batch_size=128)\n",
    "    print (\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
